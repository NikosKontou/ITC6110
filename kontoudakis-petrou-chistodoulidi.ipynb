{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acde9f5-b541-4c31-a4f1-a56b2196d701",
   "metadata": {},
   "source": [
    "# Team Project for ITC 6110 (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb9de5-bf76-4709-8d87-96f11d2f2e63",
   "metadata": {},
   "source": [
    "## Chistodoulidi Aspasia, Kontoudakis Nikos, Petrou Sofia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3ff8e5-1932-4116-94e3-2bfa250619f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/lib/python3.13/site-packages') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd083f88-126b-45a7-ae42-f49326b35cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e612c8f-872a-4db5-91d7-c4330a132aae",
   "metadata": {},
   "source": [
    "### read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18dbe87e-2d6e-4bd9-bf35-0f345bd8a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(filepath_or_buffer=\"fake_or_real_news.csv\", header=0, names= [\"id\", \"title\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0dd052-3612-471a-a2a2-5bfeec5d68db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary‚Äôs Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>‚Äî Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0   8476                       You Can Smell Hillary‚Äôs Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  ‚Äî Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2a092-73f3-4ad3-9fa1-3db2f2615442",
   "metadata": {},
   "source": [
    "# Data cleaning and quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452632fc-2799-4e24-a434-8a02e611ae3c",
   "metadata": {},
   "source": [
    "## remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7deab912-a0c7-417b-adb8-822d16f36fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 rows were removed\n"
     ]
    }
   ],
   "source": [
    "only_whitespace = original_data.map(lambda x: isinstance(x, str) and x.strip() == '')\n",
    "rows_any_whitespace = only_whitespace.any(axis=1)\n",
    "# print removed rows\n",
    "# print(original_data[rows_any_whitespace][\"text\"])\n",
    "\n",
    "print(f\"{len(original_data[rows_any_whitespace])} rows were removed\")\n",
    "original_data = original_data[~rows_any_whitespace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b6500e-acee-4a2a-a9bd-2b73dbd0cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6299 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      6299 non-null   int64 \n",
      " 1   title   6299 non-null   object\n",
      " 2   text    6299 non-null   object\n",
      " 3   label   6299 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 246.1+ KB\n"
     ]
    }
   ],
   "source": [
    "original_data.isnull().sum()\n",
    "original_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a8930-2aad-42ab-9ec7-24aae4e7f7cd",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6bdc2c-e348-496c-9505-0ac7614be682",
   "metadata": {},
   "source": [
    "# combine the title and text column into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c7cfbf-e512-40ef-be87-9daab9c22050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_text_df = original_data[[\"title\", \"text\"]]\n",
    "title_text_df = original_data[\"title\"]+ \" \"+ original_data[\"text\"]\n",
    "target=original_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a0f737-0dcb-4707-958a-ba63ca3a824c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6299"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title_text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23c6f3-b557-492e-b822-cf53be4993cf",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ce4657-1cec-430f-9d4e-7c2af826c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    # Convert text to lowercase.\n",
    "    return str(text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9512ec3b-13c8-4336-ae67-edb8df4a088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lowercase function to all summaries\n",
    "title_text_df = title_text_df.apply(lowercase_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ad1a5-64cc-48aa-9ef1-2f019bb75dd3",
   "metadata": {},
   "source": [
    "## Replace URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ff965d-2cf7-4c72-84cc-0c73f1276373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Importing the regular expressions module\n",
    "\n",
    "# Define a regex pattern to identify URLs in the text\n",
    "url_pattern = r\"(?:https?|ftp)://[^\\s/$.?#].[^\\s]*\"\n",
    "\n",
    "def replace_urls(text):\n",
    "    \"\"\"\n",
    "    Replace URLs in the text with the token 'URL'.\n",
    "    Prints before and after if a replacement occurs.\n",
    "    \"\"\"\n",
    "    text_str = str(text)\n",
    "    replaced_text = re.sub(url_pattern, 'URL', text_str)\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ad5b0a-f787-42c9-8fd3-c1fdd8a3bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply URL replacement to all summaries\n",
    "title_text_df = title_text_df.apply(replace_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22188f1d-aa82-42b3-956b-592514161717",
   "metadata": {},
   "source": [
    "## Replacing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0a9c82-3c4b-4e53-bc99-af8b966106af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# re.compile will compile the regex pattern into a regex object, necessary for \n",
    "# efficient pattern matching. This creates a reusable pattern object that can be\n",
    "# used multiple times without recompiling the pattern each time, improving performance.\n",
    "# u stands for Unicode\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "\n",
    "    # Emoticons (e.g., üòÄüòÅüòÇü§£üòÉüòÑüòÖüòÜ)\n",
    "    u\"\\U0001F600-\\U0001F64F\"  \n",
    "\n",
    "    # Symbols & pictographs (e.g., üî•üéâüí°üì¶üì±)\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  \n",
    "\n",
    "    # Transport & map symbols (e.g., üöó‚úàÔ∏èüöÄüöâ)\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  \n",
    "\n",
    "    # Flags (e.g., üá∫üá∏üá¨üáßüá®üá¶ ‚Äî these are pairs of regional indicators)\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "\n",
    "    # Dingbats (e.g., ‚úÇÔ∏è‚úàÔ∏è‚úâÔ∏è‚öΩ)\n",
    "    u\"\\u2700-\\u27BF\"          \n",
    "\n",
    "    # Supplemental Symbols & Pictographs (e.g., ü§ñü•∞üß†ü¶æ)\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  \n",
    "\n",
    "    # Symbols & Pictographs Extended-A (e.g., ü™Ñü™Öü™®)\n",
    "    u\"\\U0001FA70-\\U0001FAFF\"  \n",
    "\n",
    "    # Miscellaneous symbols (e.g., ‚òÄÔ∏è‚òÅÔ∏è‚òÇÔ∏è‚ö°)\n",
    "    u\"\\u2600-\\u26FF\"          \n",
    "\n",
    "    \"]+\", flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4852b52-84d2-4f2f-b210-a5daecce574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pattern will match common text-based emoticons that aren't covered by the emoji Unicode ranges\n",
    "# These emoticons are made up of regular ASCII characters like colons, parentheses, etc.\n",
    "\n",
    "emoticon_pattern = re.compile(r'(:\\)|:\\(|:D|:P|;\\)|:-\\)|:-D|:-P|:\\'\\(|:\\||:\\*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f4b5f3-d659-4a02-af54-cccc3eb92090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_and_print(text):\n",
    "    if emoji_pattern.search(text) or emoticon_pattern.search(text):\n",
    "        print(f\"Before: {text}\")\n",
    "        text = emoji_pattern.sub('', text)\n",
    "        text = emoticon_pattern.sub('', text)\n",
    "        print(f\"After: {text}\")\n",
    "        print()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d2876-0c9b-40f0-b76a-89f75fc61b1a",
   "metadata": {},
   "source": [
    "## Replacing Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ecdb99a-0e90-4189-ab04-85363d22fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_usernames(text):\n",
    "    \"\"\"\n",
    "    Replace email addresses and true @usernames with 'USER'.\n",
    "    Avoid matching embedded @ in profanity or stylized words.\n",
    "    Print before and after if replacement occurs.\n",
    "    \"\"\"\n",
    "    original = str(text)\n",
    "    updated = original\n",
    "\n",
    "    # Replace full email addresses\n",
    "    updated = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', 'USER', updated)\n",
    "\n",
    "    # Replace @usernames only when preceded by space, punctuation, or start of string\n",
    "    updated = re.sub(r'(?:(?<=^)|(?<=[\\s.,;!?]))@\\w+\\b', 'USER', updated)\n",
    "\n",
    "    return updated\n",
    "\n",
    "# Apply username replacement to all summaries\n",
    "title_text_df = title_text_df.apply(replace_usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e7ea8-f5a6-4851-9bb3-438c4473f1b6",
   "metadata": {},
   "source": [
    "## Removing Non-Alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c1ba0b-a720-4b5e-b623-daadb54b2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text, keep_punct=False):\n",
    "    \"\"\"\n",
    "    Clean and normalize text for NLP classification tasks.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to be cleaned.\n",
    "    - keep_punct (bool): \n",
    "        If True, retains key punctuation (. ! ?) which may carry emotional or contextual weight.\n",
    "        If False, removes all non-alphabetic characters for simpler lexical analysis.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The cleaned text string, lowercased and stripped of unwanted characters.\n",
    "    \n",
    "    This function is designed for flexibility across different NLP tasks like sentiment analysis,\n",
    "    topic classification, or spam detection. It handles:\n",
    "    - Lowercasing text for normalization\n",
    "    - Removing or preserving select punctuation\n",
    "    - Removing digits, symbols, and special characters\n",
    "    - Reducing multiple spaces to a single space\n",
    "    - Optionally printing changes for debugging or logging\n",
    "\n",
    "    When to use `keep_punct=True`:\n",
    "    - Sentiment analysis: punctuation (e.g., \"!\", \"?\") can reflect strong emotion\n",
    "    - Social media or informal text: expressive punctuation often carries signal\n",
    "    - Sarcasm, emphasis, or tone-sensitive tasks\n",
    "\n",
    "    When to use `keep_punct=False`:\n",
    "    - Topic classification or document clustering: punctuation rarely adds value\n",
    "    - Preprocessing for bag-of-words, TF-IDF, or topic modeling\n",
    "    - When punctuation is inconsistent or noisy (e.g., OCR scans, scraped data)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert input to string (safe handling)\n",
    "    original = str(text)\n",
    "\n",
    "    if keep_punct:\n",
    "        # Keep only lowercase letters, spaces, and select punctuation (. ! ?)\n",
    "        # Useful for capturing tone/sentiment\n",
    "        cleaned = re.sub(r\"[^a-z\\s.!?]\", \"\", original)\n",
    "    else:\n",
    "        # Keep only lowercase letters and spaces; remove all punctuation and symbols\n",
    "        cleaned = re.sub(r\"[^a-z\\s]\", \"\", original)\n",
    "\n",
    "    # Normalize whitespace (collapse multiple spaces to one, strip leading/trailing)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "    # Optional: print before and after if a change occurred\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bdfcdad-ddcc-4f99-bf22-a0fa45447b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply non-alphabet removal to all summaries\n",
    "title_text_df = title_text_df.apply(lambda x: clean_text(x, keep_punct=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0092f-097e-4a1d-9198-48256044cd31",
   "metadata": {},
   "source": [
    "## Removing Consecutive letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46c7062d-60c7-4af0-9b79-b85d86e361f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_letters(text, max_repeat=2):\n",
    "    \"\"\"\n",
    "    Normalize elongated words by limiting repeated characters.\n",
    "\n",
    "    In informal or emotional text (e.g., reviews, tweets), users often repeat letters\n",
    "    to add emphasis: \"sooooo good\", \"loooove it\", \"greeaaat\".\n",
    "    \n",
    "    This function reduces any character repeated more than `max_repeat` times \n",
    "    to exactly `max_repeat` occurrences (default: 2), preserving emphasis without bloating vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text\n",
    "    - max_repeat (int): The maximum allowed repetitions for any character\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with repeated characters normalized\n",
    "    \"\"\"\n",
    "    text_str = str(text)\n",
    "    pattern = r'(\\w)\\1{' + str(max_repeat) + r',}'\n",
    "    cleaned = re.sub(pattern, r'\\1' * max_repeat, text_str)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "660621dd-1870-428d-bc29-c956313e2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply consecutive letter removal to all summaries\n",
    "title_text_df = title_text_df.apply(lambda x: remove_consecutive_letters(x, max_repeat=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971431bd-f6f6-4e02-99ec-0890f7da724e",
   "metadata": {},
   "source": [
    "## Removing Short Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76042928-7804-4ec9-b1fb-0eec54ab4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(text, min_length=3, preserve_words=None):\n",
    "    \"\"\"\n",
    "    Remove short words from text based on a minimum length threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text\n",
    "    - min_length (int): Minimum word length to keep (default = 3)\n",
    "    - preserve_words (set or list): Optional set of short but important words to keep (e.g., {'no', 'not'})\n",
    "    \n",
    "    Returns:\n",
    "    - str: Text with short words removed, except for preserved ones\n",
    "    \n",
    "    Notes:\n",
    "    - Use with care in sentiment analysis. Important short words like 'no', 'not', 'bad' may affect meaning.\n",
    "    - Best used after stopword removal or on very noisy text.\n",
    "    \"\"\"\n",
    "    preserve = set(preserve_words or [])\n",
    "    words = str(text).split()\n",
    "    filtered = [word for word in words if len(word) >= min_length or word.lower() in preserve]\n",
    "    result = ' '.join(filtered)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a06ba26e-bb10-4dac-9852-e4b85846dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply short word removal to all summaries\n",
    "title_text_df = title_text_df.apply(lambda x: remove_short_words(x, min_length=3, preserve_words={'no', 'not'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a203f-3001-435a-aebc-d8655ea290d9",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5e8b637-5ecf-451d-93e4-86c1b3934a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample stopwords: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "# NLTK (Natural Language Toolkit) is a popular library for natural language processing in Python\n",
    "# https://www.nltk.org/\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"Sample stopwords:\", list(stopwords.words('english'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3f491e0-2c87-4cc5-9cc6-e66ce6b9037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords but keep critical ones like \"not\"\n",
    "base_stopwords = set(stopwords.words('english'))\n",
    "preserve = {'no', 'not', 'nor', 'never'}\n",
    "custom_stopwords = base_stopwords - preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6685087-2709-4c5e-9c70-ba7af26477ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords from text, preserving key negation words.\n",
    "\n",
    "    This function uses a customized stopword list that retains important\n",
    "    short words like 'not', 'no', 'nor', and 'never' which carry significant\n",
    "    meaning in tasks like sentiment analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): Lowercased input text\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with stopwords removed, but critical negation words preserved\n",
    "    \"\"\"\n",
    "    words = str(text).split()\n",
    "    filtered = [word for word in words if word not in custom_stopwords]\n",
    "    result = ' '.join(filtered)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f6117c0-4da2-4ec6-925d-3850f210513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply remove_stopwords\n",
    "title_text_df = title_text_df.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30542a-34aa-4c4c-932f-c1b77922654c",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4468137c-6afa-4d46-8691-71276e9c062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nikos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nikos/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/nikos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('wordnet')  # Download WordNet, a lexical database of English words\n",
    "nltk.download('omw-1.4')  # WordNet Lemmas sometimes need this, which is a mapping of WordNet lemmas to their Part of Speech (POS) tags.\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # Download English POS tagger\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "872b5ce1-fb6a-47bd-8148-f7f2aaa64215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS mapping function \n",
    "# POS tags can be: ADJ (adjective), ADV (adverb), NOUN (noun), VERB (verb), etc\n",
    "def get_wordnet_pos(tag):\n",
    "    # Determine the WordNet POS tag based on the first letter of the input tag\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to Noun if no match\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize text using WordNet lemmatizer with POS tagging.\n",
    "\n",
    "    This version prints each change along with the POS tag of the changed word.\n",
    "    \"\"\"\n",
    "    # Convert the input text to a string to ensure compatibility\n",
    "    original_text = str(text)\n",
    "    # Split the text into individual words\n",
    "    words = original_text.split()\n",
    "    # Obtain Part of Speech (POS) tags for each word\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Initialize lists to store lemmatized words and any changes\n",
    "    lemmatized_words = []\n",
    "    changes = []\n",
    "\n",
    "    # Iterate over each word and its POS tag\n",
    "    for word, tag in pos_tags:\n",
    "        # Map the POS tag to a WordNet POS tag\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        # Lemmatize the word using the mapped POS tag\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "\n",
    "        # Check if the lemmatized word is different from the original\n",
    "        if lemma != word:\n",
    "            # Record the change if a difference is found\n",
    "            changes.append((word, lemma, tag))\n",
    "        # Add the lemmatized word to the list\n",
    "        lemmatized_words.append(lemma)\n",
    "\n",
    "    # Join the lemmatized words back into a single string\n",
    "    result = ' '.join(lemmatized_words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72a84f2a-f568-48e1-802d-96bee51091e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       smell hillary fear daniel greenfield shillman ...\n",
      "1       watch exact moment paul ryan commit political ...\n",
      "2       kerry paris gesture sympathy u.s. secretary st...\n",
      "3       bernie supporter twitter erupt anger dnc try w...\n",
      "4       battle new york primary matter primary day new...\n",
      "                              ...                        \n",
      "6330    state department say cant find emails clinton ...\n",
      "6331    pb stand plutocratic pentagon pb stand plutocr...\n",
      "6332    antitrump protester tool oligarchy information...\n",
      "6333    ethiopia obama seek progress peace security ea...\n",
      "6334    jeb bush suddenly attack trump. here matter je...\n",
      "Length: 6299, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatization to all summaries\n",
    "title_text_df = title_text_df.apply(lemmatize_text)\n",
    "print(title_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00742639-1d07-4bc8-9f4d-2d77adab3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text_df.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
